<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible">
    <title>Bibek's Webpage</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans|Roboto+Slab" rel="stylesheet">
    <style>
        body{
            font-family: 'Roboto Slab', serif;
        }
        ul{
            font-family: 'Open Sans', sans-serif;
        }
        
    </style>
    <link rel="stylesheet" type="text/css" media="screen" href="main.css" />
    <script src="main.js"></script>
</head>
<body>
    <div>
        <h1>Assignment: 2</h1>
        <h3>Name: Bibek K Pandit</h3>
        <h3>Email: bibek@mit.edu</h3>
        <hr>
        <div>
            <ul>
                <li><b>(1.1) You should see in the leftmost column of Model Builder that the initial model is marked “Invalid model”. Why is the model invalid?</b></li>
                <li>This model is invalid because the model has a 2 dimensional input layer (non-flattened) mapping to 1 dimensional output layer. That is not allowed.</li>
                <br>

                <li><b>(1.1) The classifications you are seeing are almost always wrong. Why is this? What performance should you expect from this particular network, i.e., how often should you expect it to be correct? Is this what you observe?</b></li>
                <li>The classification is wrong because the weights have been initialized to random values and those random values are being used to perform predictions. Thus, the classification is very random and mostly wrong. The network should get 1 prediction right out of every 10 predictions it performs. That is the result we observe roughly.</li>
                <br>

                <li><b>(1.3) 1. What accuracy do you observe in training MNIST? How many inferences per second does the demo perform? How many examples per second does it train? Then try the same thing with Fashion MNIST and document your findings.</b></li>
                <li>The training accuracy as well as the test accuracy are around 84% on the MNIST data. The demo performs 1040 inferences per second. It trains 466 examples per second. Both the training accuracy and the test accuracy reach 80% on the Fashion MNIST data. The demo performs around 600 inferences per second. It trains around 400 examples per second.</li>
                <br>

                <li><b>(1.3) 2. Change the Dataset to CIFAR-10. This will take about 30 seconds to load, due to the large number of images. Change the model back to Custom and add the flatten and fully connected layers as above. What accuracy do you observe in training CIFAR-10 after letting it train for a minute or two. You should find that it’s a lot worse than for MNIST. We’ll talk about why performance is bad when we discuss convolutional networks.</b></li>
                <li>Both the training accuracy and the test accuracy reach 38% on the CIFR-10 data. The demo performs around 650 inferences per second. It trains around 420 examples per second.</li>
                <br>

                <li><b>(1.3) 3. Changing back to MNIST, let’s consider a simple idea for improving accuracy, which turns out not to work: just add more fully connected units, one on top of the other. Start training and you should see the accuracy plummet to zero, with terrible results. What’s going on?</b></li>
                <li>I think that we are getting exploding gradient problems here. Since there is no activation function, the gradient simply flow from one layer to the other. This very quickly builds up the weights (weight values become very high). So the predictions do not improve and we get Nans as output probabilities.</li>
                <br>

                <li><b>(1.4) Return to the MNIST model with two FC layers you tried above and add a ReLU layer between the FC layers so that the sequence of layers becomes:
                    Input → Flatten → FC(10) → ReLU → FC(10) → Softmax → Label
                    Train the new model. How well does it perform? Then make the first FC model wider by increasing the number of units to 100. Does this make a difference? Document the results for these questions on your webpage.
                </b></li>
                <li>Addition of the ReLU solves the exploding gradients problem (we do not get NaNs anymore). However, with just 10 units in the first FC layer, the accuracy is just above 15% (not much better than random. (If we change the optimizer to adam, we can get 60% accuracy on training set and test set). Increasing the number of units to 100 in the first FC increases the accuracy to around 90% (optimizer is momentum). All this makes sense since with more units there are more parameters to tune. Thus, better accuracy can be achieved.</li>
            </ul>
        </div>
    </div>
    
    
    
</body>
</html>